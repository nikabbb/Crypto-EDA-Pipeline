Project Documentation: Binance Data Analysis Dashboard
1. Introduction

This project aims to create a comprehensive data analysis dashboard for Binance cryptocurrency data. It involves generating synthetic data, building an ETL (Extract, Transform, Load) pipeline, and visualizing the data using a Streamlit web application. The dashboard provides insights into transaction patterns, user behavior, market trends, and network activity.

2. Project Objectives

Generate realistic synthetic data for Binance transactions, users, market, and network activities.
Develop an ETL pipeline to clean, transform, and load the generated data into a suitable format for analysis.
Create an interactive Streamlit dashboard to visualize and analyze the transformed data.
Provide business-insightful visualizations to support informed decision-making.
3. Project Structure

The project consists of the following components:

etl.py: Contains the code for data generation and the ETL pipeline.
streamlit_app.py: Contains the code for the Streamlit web application and visualizations.
Parquet Files (transformed_*.parquet): Stores the transformed data from the ETL pipeline.
4. Data Generation

The synthetic data is generated using Python and libraries like pandas and numpy. The etl.py script generates data for four primary datasets:

Transactions: Includes transaction details like timestamp, user addresses, token symbols, amount, fees, etc.
Users: Contains user information like registration date, last login time, account balance, country, device type, and activity status.
Market: Captures market data such as trading pair, timestamp, open/close/high/low prices, volume, and market cap.
Network: Includes network-related data like timestamp, gas prices, transaction per second, block time, difficulty, and hashrate.
The data generation process involves creating realistic distributions and relationships between the variables to simulate real-world Binance data.

5. ETL Pipeline

The ETL pipeline is implemented in the etl.py script. It consists of the following steps:

Extraction: Load the generated data into Pandas DataFrames.
Transformation:
Handle missing values using imputation or removal.
Convert data types to appropriate formats (e.g., datetime).
Calculate derived features (e.g., transaction fees in USD, days since registration).
Handle outliers using techniques like capping or removal.
Create new columns such as hour, day of the week, and other time related columns.
Loading: Save the transformed DataFrames into Parquet files for efficient storage and retrieval.
6. Streamlit Dashboard

The Streamlit dashboard is built using the streamlit_app.py script. It provides an interactive interface to visualize and analyze the transformed data.

Data Loading: The transformed Parquet files are loaded into Pandas DataFrames using the load_data() function.
Visualizations: Plotly Express is used to create interactive charts and graphs, including:
Transaction volume over time.
Transaction type distribution.
User registration trends.
Market price trends.
Network gas price trends.
Top traders by volume.
Average transaction fee by token.
User activity over time.
Market volatility.
Correlation heatmaps.
Distribution of account balances.
Trading volume by token.
Network difficulty over time.
Top countries by user count.
Transaction fees vs transaction amounts.
User device type distribution.
Market cap vs. volume.
Average block time over time.
Leverage distribution.
Time series decomposition.
Rolling averages and volatility.
User segmentation.
Feature importance.
Time series analysis of gas prices.
Layout: The dashboard is designed to be responsive, using st.columns to arrange visualizations in multiple columns on larger screens.
Interactivity: Streamlit widgets like st.selectbox and st.slider are used to provide interactive controls for filtering and exploring the data.
Styling: All plotly graphs are set to the color yellow.
7. Libraries Used

streamlit: For creating the web application.
pandas: For data manipulation and analysis.
plotly.express: For creating interactive visualizations.
numpy: For numerical operations.
statsmodels: For statistical modeling and time series analysis.
scikit-learn: For machine learning models (Random Forest Regressor).
matplotlib: for time series analysis graphs.
8. Deployment (Optional)

The Streamlit dashboard can be deployed to platforms like Streamlit Sharing, Heroku, or AWS to make it accessible online.

